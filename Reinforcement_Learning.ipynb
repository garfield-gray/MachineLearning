{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWGzyXrCtY9W4bQ5VSP6oj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/garfield-gray/MachineLearning/blob/main/Reinforcement_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Tram Problem"
      ],
      "metadata": {
        "id": "kHy4v194dT6S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2xvBLiupdMEZ"
      },
      "outputs": [],
      "source": [
        "class TransportationMDP(object):\n",
        "    def __init__(self, N):\n",
        "        # N = number of states\n",
        "        self.N = N\n",
        "    def startstate(self):\n",
        "        return 1\n",
        "    def is_final(self, state):\n",
        "        return state == self.N\n",
        "    def actions(self, state):\n",
        "        result = []\n",
        "        if state+1<=self.N:\n",
        "            result.append('walk')\n",
        "        if state*2<=self.N:\n",
        "            result.append('tram')\n",
        "        return result\n",
        "    def succProbReward(self, state, action):\n",
        "        # return list of (newState, prob, reward) triples\n",
        "        # state = s, action = a, newState = s'\n",
        "        # prob = T(s, a, s'), reward = Reward(s, a, s')\n",
        "        result = []\n",
        "        if action=='walk':\n",
        "            result.append((state+1, 1.0, -1.0))\n",
        "        else:\n",
        "            failProb = 0.1\n",
        "            result.append((state*2, 1-failProb, -2.0))\n",
        "            result.append((state,   failProb, -2.0))\n",
        "        return result\n",
        "    def discount(self):\n",
        "        return 1.0\n",
        "\n",
        "    def states(self):\n",
        "        return range(1, self.N+1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mdp = TransportationMDP(N=10)\n",
        "mdp.actions(5)\n",
        "mdp.succProbReward(5, 'tram')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3LSG-QhdaCu",
        "outputId": "b96f30de-4a72-4012-9165-bd783c3c2612"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(10, 0.9, -2.0), (5, 0.1, -2.0)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def value_iteration(mdp):\n",
        "    # initialize\n",
        "    V = {} # state -> estimate of optimal value function\n",
        "    for state in mdp.states():\n",
        "        V[state] = 0.0\n",
        "\n",
        "    def Q(state, action):\n",
        "        return sum(prob*(reward+mdp.discount()*V[newState]) for newState, prob, reward in mdp.succProbReward(state, action))\n",
        "    while True:\n",
        "        # compute\n",
        "        newV = {}\n",
        "        for state in mdp.states():\n",
        "            # v* = max_a sum_s' T(s, a, s') * [R(s, a, s') + gamma\n",
        "            if mdp.is_final(state):\n",
        "                newV[state] = 0.0\n",
        "            else:\n",
        "                newV[state] = max(Q(state, action) for action in mdp.actions(state))\n",
        "\n",
        "        # check convergence\n",
        "        if max(abs(V[state]-newV[state]) for state in mdp.states())<1e-10:\n",
        "            break\n",
        "        # print(V)\n",
        "        V = newV\n",
        "\n",
        "        pi = {}\n",
        "        for state in mdp.states():\n",
        "            if mdp.is_final(state):\n",
        "                pi[state] = 'final'\n",
        "            else:\n",
        "                pi[state] = max((Q(state, action), action) for action in mdp.actions(state))[1]\n",
        "        # print\n",
        "        os.system('clear')\n",
        "        print('{:15} {:15} {:15}'.format('State', 'Optimal Value', 'Optimal Action'))\n",
        "        for state in mdp.states():\n",
        "            print('{:15} {:15} {:15}'.format(state, V[state], pi[state]))\n",
        "        input()\n",
        "\n"
      ],
      "metadata": {
        "id": "O_w3B6mCdaIK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mdp = TransportationMDP(N=10)\n",
        "value_iteration(mdp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZEd77IddaND",
        "outputId": "1a004d68-ae79-43df-91b2-d722a300d08a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "State           Optimal Value   Optimal Action \n",
            "              1            -1.0 walk           \n",
            "              2            -1.0 walk           \n",
            "              3            -1.0 walk           \n",
            "              4            -1.0 walk           \n",
            "              5            -1.0 walk           \n",
            "              6            -1.0 walk           \n",
            "              7            -1.0 walk           \n",
            "              8            -1.0 walk           \n",
            "              9            -1.0 walk           \n",
            "             10             0.0 final          \n",
            "\n",
            "State           Optimal Value   Optimal Action \n",
            "              1            -2.0 walk           \n",
            "              2            -2.0 walk           \n",
            "              3            -2.0 walk           \n",
            "              4            -2.0 walk           \n",
            "              5            -2.0 tram           \n",
            "              6            -2.0 walk           \n",
            "              7            -2.0 walk           \n",
            "              8            -2.0 walk           \n",
            "              9            -1.0 walk           \n",
            "             10             0.0 final          \n",
            "\n",
            "State           Optimal Value   Optimal Action \n",
            "              1            -3.0 walk           \n",
            "              2            -3.0 walk           \n",
            "              3            -3.0 walk           \n",
            "              4            -3.0 walk           \n",
            "              5            -2.2 tram           \n",
            "              6            -3.0 walk           \n",
            "              7            -3.0 walk           \n",
            "              8            -2.0 walk           \n",
            "              9            -1.0 walk           \n",
            "             10             0.0 final          \n",
            "\n",
            "State           Optimal Value   Optimal Action \n",
            "              1            -4.0 walk           \n",
            "              2            -4.0 walk           \n",
            "              3            -4.0 walk           \n",
            "              4            -3.2 walk           \n",
            "              5           -2.22 tram           \n",
            "              6            -4.0 walk           \n",
            "              7            -3.0 walk           \n",
            "              8            -2.0 walk           \n",
            "              9            -1.0 walk           \n",
            "             10             0.0 final          \n",
            "\n",
            "State           Optimal Value   Optimal Action \n",
            "              1            -5.0 walk           \n",
            "              2            -5.0 walk           \n",
            "              3            -4.2 walk           \n",
            "              4           -3.22 walk           \n",
            "              5          -2.222 tram           \n",
            "              6            -4.0 walk           \n",
            "              7            -3.0 walk           \n",
            "              8            -2.0 walk           \n",
            "              9            -1.0 walk           \n",
            "             10             0.0 final          \n",
            "\n",
            "\n",
            "State           Optimal Value   Optimal Action \n",
            "              1            -6.0 walk           \n",
            "              2            -5.2 walk           \n",
            "              3 -4.220000000000001 walk           \n",
            "              4          -3.222 walk           \n",
            "              5         -2.2222 tram           \n",
            "              6            -4.0 walk           \n",
            "              7            -3.0 walk           \n",
            "              8            -2.0 walk           \n",
            "              9            -1.0 walk           \n",
            "             10             0.0 final          \n",
            "State           Optimal Value   Optimal Action \n",
            "              1            -6.2 walk           \n",
            "              2 -5.220000000000001 walk           \n",
            "              3 -4.2219999999999995 walk           \n",
            "              4         -3.2222 walk           \n",
            "              5        -2.22222 tram           \n",
            "              6            -4.0 walk           \n",
            "              7            -3.0 walk           \n",
            "              8            -2.0 walk           \n",
            "              9            -1.0 walk           \n",
            "             10             0.0 final          \n",
            "\n",
            "\n",
            "State           Optimal Value   Optimal Action \n",
            "              1 -6.220000000000001 walk           \n",
            "              2 -5.2219999999999995 walk           \n",
            "              3         -4.2222 walk           \n",
            "              4        -3.22222 walk           \n",
            "              5       -2.222222 tram           \n",
            "              6            -4.0 walk           \n",
            "              7            -3.0 walk           \n",
            "              8            -2.0 walk           \n",
            "              9            -1.0 walk           \n",
            "             10             0.0 final          \n",
            "State           Optimal Value   Optimal Action \n",
            "              1 -6.2219999999999995 walk           \n",
            "              2         -5.2222 walk           \n",
            "              3        -4.22222 walk           \n",
            "              4       -3.222222 walk           \n",
            "              5      -2.2222222 tram           \n",
            "              6            -4.0 walk           \n",
            "              7            -3.0 walk           \n",
            "              8            -2.0 walk           \n",
            "              9            -1.0 walk           \n",
            "             10             0.0 final          \n",
            "\n",
            "State           Optimal Value   Optimal Action \n",
            "              1         -6.2222 walk           \n",
            "              2        -5.22222 walk           \n",
            "              3       -4.222222 walk           \n",
            "              4      -3.2222222 walk           \n",
            "              5     -2.22222222 tram           \n",
            "              6            -4.0 walk           \n",
            "              7            -3.0 walk           \n",
            "              8            -2.0 walk           \n",
            "              9            -1.0 walk           \n",
            "             10             0.0 final          \n",
            "\n",
            "State           Optimal Value   Optimal Action \n",
            "              1        -6.22222 walk           \n",
            "              2       -5.222222 walk           \n",
            "              3      -4.2222222 walk           \n",
            "              4     -3.22222222 walk           \n",
            "              5    -2.222222222 tram           \n",
            "              6            -4.0 walk           \n",
            "              7            -3.0 walk           \n",
            "              8            -2.0 walk           \n",
            "              9            -1.0 walk           \n",
            "             10             0.0 final          \n",
            "\n",
            "State           Optimal Value   Optimal Action \n",
            "              1       -6.222222 walk           \n",
            "              2      -5.2222222 walk           \n",
            "              3     -4.22222222 walk           \n",
            "              4    -3.222222222 walk           \n",
            "              5   -2.2222222222 tram           \n",
            "              6            -4.0 walk           \n",
            "              7            -3.0 walk           \n",
            "              8            -2.0 walk           \n",
            "              9            -1.0 walk           \n",
            "             10             0.0 final          \n",
            "\n",
            "State           Optimal Value   Optimal Action \n",
            "              1      -6.2222222 walk           \n",
            "              2     -5.22222222 walk           \n",
            "              3    -4.222222222 walk           \n",
            "              4   -3.2222222222 walk           \n",
            "              5  -2.22222222222 tram           \n",
            "              6            -4.0 walk           \n",
            "              7            -3.0 walk           \n",
            "              8            -2.0 walk           \n",
            "              9            -1.0 walk           \n",
            "             10             0.0 final          \n",
            "\n",
            "State           Optimal Value   Optimal Action \n",
            "              1     -6.22222222 walk           \n",
            "              2    -5.222222222 walk           \n",
            "              3   -4.2222222222 walk           \n",
            "              4  -3.22222222222 walk           \n",
            "              5 -2.2222222222220003 tram           \n",
            "              6            -4.0 walk           \n",
            "              7            -3.0 walk           \n",
            "              8            -2.0 walk           \n",
            "              9            -1.0 walk           \n",
            "             10             0.0 final          \n",
            "\n",
            "State           Optimal Value   Optimal Action \n",
            "              1    -6.222222222 walk           \n",
            "              2   -5.2222222222 walk           \n",
            "              3  -4.22222222222 walk           \n",
            "              4 -3.2222222222220003 walk           \n",
            "              5 -2.2222222222222 tram           \n",
            "              6            -4.0 walk           \n",
            "              7            -3.0 walk           \n",
            "              8            -2.0 walk           \n",
            "              9            -1.0 walk           \n",
            "             10             0.0 final          \n",
            "\n",
            "State           Optimal Value   Optimal Action \n",
            "              1   -6.2222222222 walk           \n",
            "              2  -5.22222222222 walk           \n",
            "              3 -4.222222222222 walk           \n",
            "              4 -3.2222222222222 walk           \n",
            "              5 -2.22222222222222 tram           \n",
            "              6            -4.0 walk           \n",
            "              7            -3.0 walk           \n",
            "              8            -2.0 walk           \n",
            "              9            -1.0 walk           \n",
            "             10             0.0 final          \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "min((3, 'walk'), (2, 'tram'), (4, 'walk'), (6, 'tram'), (10, 'walk'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZ3Y62DSdaRI",
        "outputId": "0cd11510-bb2b-4267-a412-b2a83d3733c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 'tram')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max((3, 'walk'), (10, 'tram'), (4, 'walk'), (6, 'tram'), (10, 'walk'))"
      ],
      "metadata": {
        "id": "T5Gwk53JbGq-",
        "outputId": "3887b010-5239-4aac-c8cb-5ae75bb7ae4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 'walk')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Volcano"
      ],
      "metadata": {
        "id": "87zrp1Qsdaz0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UNAaFfYMdczE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fdy5KWTIddOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_51QzV2wddVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WpbVnSsbddbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# who knows?!"
      ],
      "metadata": {
        "id": "oO9RwbWadfPg"
      }
    }
  ]
}