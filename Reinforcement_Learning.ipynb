{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQ8NceSnKW4ZwdKr8RELUm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/garfield-gray/MachineLearning/blob/main/Reinforcement_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Tram Problem"
      ],
      "metadata": {
        "id": "kHy4v194dT6S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "2xvBLiupdMEZ"
      },
      "outputs": [],
      "source": [
        "class TransportationMDP(object):\n",
        "    def __init__(self, N):\n",
        "        # N = number of states\n",
        "        self.N = N\n",
        "    def startstate(self):\n",
        "        return 1\n",
        "    def is_final(self, state):\n",
        "        return state == self.N\n",
        "    def actions(self, state):\n",
        "        result = []\n",
        "        if state+1<=self.N:\n",
        "            result.append('walk')\n",
        "        if state*2<=self.N:\n",
        "            result.append('tram')\n",
        "        return result\n",
        "    def succProbReward(self, state, action):\n",
        "        # return list of (newState, prob, reward) triples\n",
        "        # state = s, action = a, newState = s'\n",
        "        # prob = T(s, a, s'), reward = Reward(s, a, s')\n",
        "        result = []\n",
        "        if action=='walk':\n",
        "            result.append((state+1, 1.0, -1.0))\n",
        "        else:\n",
        "            result.append((state*2, 0.5, -2.0))\n",
        "            result.append((state*2, 0.5, -2.0))\n",
        "        return result\n",
        "    def discount(self):\n",
        "        return 1.0\n",
        "\n",
        "    def states(self):\n",
        "        return range(1, self.N+1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mdp = TransportationMDP(N=10)\n",
        "mdp.actions(5)\n",
        "# mdp.states()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3LSG-QhdaCu",
        "outputId": "95cbbd1f-acc2-4478-8cdb-08a7dde1b50e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['walk', 'tram']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def value_iteration(mdp):\n",
        "    # initialize\n",
        "    V = {} # state -> estimate of optimal value function\n",
        "    for state in mdp.states():\n",
        "        V[state] = 0.0\n",
        "\n",
        "    def Q(state, action):\n",
        "        return sum(prob*(reward+mdp.discount()*V[newState]) for newState, prob, reward in mdp.succProbReward(state, action))\n",
        "    while True:\n",
        "        # compute\n",
        "        newV = {}\n",
        "        for state in mdp.states():\n",
        "            # v* = max_a sum_s' T(s, a, s') * [R(s, a, s') + gamma\n",
        "            if mdp.is_final(state):\n",
        "                newV[state] = 0.0\n",
        "            else:\n",
        "                newV[state] = max(Q(state, action) for action in mdp.actions(state))\n",
        "\n",
        "        # check convergence\n",
        "        if max(abs(V[state]-newV[state]) for state in mdp.states())<1e-10:\n",
        "            break\n",
        "        # print(V)\n",
        "        V = newV\n",
        "\n",
        "        pi = {}\n",
        "        for state in mdp.states():\n",
        "            if mdp.is_final(state):\n",
        "                pi[state] = 'final'\n",
        "            else:\n",
        "                pi[state] = max((Q(state, action), action) for action in mdp.actions(state))[1]\n",
        "        # print\n",
        "        os.system('clear')\n",
        "        print('{:15} {:15} {:15}'.format('State', 'Optimal Value', 'Optimal Action'))\n",
        "        for state in mdp.states():\n",
        "            print('{:15} {:15} {:15}'.format(state, V[state], pi[state]))\n",
        "        input()\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "O_w3B6mCdaIK"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mdp = TransportationMDP(N=10)\n",
        "value_iteration(mdp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZEd77IddaND",
        "outputId": "59ee0955-408a-4888-b839-80c739bf864e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0}\n",
            "State           Optimal Value   Optimal Action \n",
            "              1            -1.0 walk           \n",
            "              2            -1.0 walk           \n",
            "              3            -1.0 walk           \n",
            "              4            -1.0 walk           \n",
            "              5            -1.0 walk           \n",
            "              6            -1.0 walk           \n",
            "              7            -1.0 walk           \n",
            "              8            -1.0 walk           \n",
            "              9            -1.0 walk           \n",
            "             10             0.0 final          \n",
            "4\n",
            "{1: -1.0, 2: -1.0, 3: -1.0, 4: -1.0, 5: -1.0, 6: -1.0, 7: -1.0, 8: -1.0, 9: -1.0, 10: 0.0}\n",
            "State           Optimal Value   Optimal Action \n",
            "              1            -2.0 walk           \n",
            "              2            -2.0 walk           \n",
            "              3            -2.0 walk           \n",
            "              4            -2.0 walk           \n",
            "              5            -2.0 tram           \n",
            "              6            -2.0 walk           \n",
            "              7            -2.0 walk           \n",
            "              8            -2.0 walk           \n",
            "              9            -1.0 walk           \n",
            "             10             0.0 final          \n",
            "9\n",
            "{1: -2.0, 2: -2.0, 3: -2.0, 4: -2.0, 5: -2.0, 6: -2.0, 7: -2.0, 8: -2.0, 9: -1.0, 10: 0.0}\n",
            "State           Optimal Value   Optimal Action \n",
            "              1            -3.0 walk           \n",
            "              2            -3.0 walk           \n",
            "              3            -3.0 walk           \n",
            "              4            -3.0 walk           \n",
            "              5            -2.0 tram           \n",
            "              6            -3.0 walk           \n",
            "              7            -3.0 walk           \n",
            "              8            -2.0 walk           \n",
            "              9            -1.0 walk           \n",
            "             10             0.0 final          \n",
            "0\n",
            "{1: -3.0, 2: -3.0, 3: -3.0, 4: -3.0, 5: -2.0, 6: -3.0, 7: -3.0, 8: -2.0, 9: -1.0, 10: 0.0}\n",
            "State           Optimal Value   Optimal Action \n",
            "              1            -4.0 walk           \n",
            "              2            -4.0 walk           \n",
            "              3            -4.0 walk           \n",
            "              4            -3.0 walk           \n",
            "              5            -2.0 tram           \n",
            "              6            -4.0 walk           \n",
            "              7            -3.0 walk           \n",
            "              8            -2.0 walk           \n",
            "              9            -1.0 walk           \n",
            "             10             0.0 final          \n",
            "9\n",
            "{1: -4.0, 2: -4.0, 3: -4.0, 4: -3.0, 5: -2.0, 6: -4.0, 7: -3.0, 8: -2.0, 9: -1.0, 10: 0.0}\n",
            "State           Optimal Value   Optimal Action \n",
            "              1            -5.0 walk           \n",
            "              2            -5.0 walk           \n",
            "              3            -4.0 walk           \n",
            "              4            -3.0 walk           \n",
            "              5            -2.0 tram           \n",
            "              6            -4.0 walk           \n",
            "              7            -3.0 walk           \n",
            "              8            -2.0 walk           \n",
            "              9            -1.0 walk           \n",
            "             10             0.0 final          \n",
            "8\n",
            "{1: -5.0, 2: -5.0, 3: -4.0, 4: -3.0, 5: -2.0, 6: -4.0, 7: -3.0, 8: -2.0, 9: -1.0, 10: 0.0}\n",
            "State           Optimal Value   Optimal Action \n",
            "              1            -6.0 walk           \n",
            "              2            -5.0 walk           \n",
            "              3            -4.0 walk           \n",
            "              4            -3.0 walk           \n",
            "              5            -2.0 tram           \n",
            "              6            -4.0 walk           \n",
            "              7            -3.0 walk           \n",
            "              8            -2.0 walk           \n",
            "              9            -1.0 walk           \n",
            "             10             0.0 final          \n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZ3Y62DSdaRI",
        "outputId": "5468987e-09af-4a98-c292-a535a10cea32"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "range(0, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Volcano"
      ],
      "metadata": {
        "id": "87zrp1Qsdaz0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UNAaFfYMdczE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fdy5KWTIddOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_51QzV2wddVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WpbVnSsbddbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# who knows?!"
      ],
      "metadata": {
        "id": "oO9RwbWadfPg"
      }
    }
  ]
}